# ğŸ“Š Guia de AvaliaÃ§Ã£o do RacoGraph

Este guia explica como usar o sistema de avaliaÃ§Ã£o offline para medir a qualidade das recomendaÃ§Ãµes geradas pelo RacoGraph.

---

## ğŸ¯ O que Ã© AvaliaÃ§Ã£o Offline?

A avaliaÃ§Ã£o offline permite testar o sistema de recomendaÃ§Ã£o **sem usuÃ¡rios reais**, usando dados histÃ³ricos. O processo Ã©:

1. **Dividir dados**: Separa avaliaÃ§Ãµes em TREINO e TESTE
2. **Treinar**: ConstrÃ³i o grafo usando apenas dados de TREINO
3. **Recomendar**: Gera recomendaÃ§Ãµes para cada usuÃ¡rio
4. **Avaliar**: Compara recomendaÃ§Ãµes com o que o usuÃ¡rio **realmente** fez no TESTE

---

## ğŸš€ Como Usar

### Uso BÃ¡sico

```bash
python eval.py
```

Executa avaliaÃ§Ã£o padrÃ£o:
- Top-10 recomendaÃ§Ãµes
- 1000 caminhadas aleatÃ³rias
- Ãšltimo filme de cada usuÃ¡rio no teste
- Nota mÃ­nima de 3.0

### Exemplos PrÃ¡ticos

#### 1ï¸âƒ£ Avaliar Top-20 recomendaÃ§Ãµes
```bash
python eval.py --k 20
```

#### 2ï¸âƒ£ Aumentar precisÃ£o (mais caminhadas)
```bash
python eval.py --num-walks 5000
```

#### 3ï¸âƒ£ Caminhadas mais longas
```bash
python eval.py --walk-length 15
```

#### 4ï¸âƒ£ Considerar apenas notas altas
```bash
python eval.py --min-user-rating 4.0
```

#### 5ï¸âƒ£ Split aleatÃ³rio (20% teste)
```bash
python eval.py --split random --test-frac 0.2
```

#### 6ï¸âƒ£ AvaliaÃ§Ã£o completa otimizada
```bash
python eval.py --k 10 --num-walks 2000 --walk-length 15 --min-user-rating 3.5
```

---

## ğŸ“Š Entendendo as MÃ©tricas

### ğŸ¯ Precision@K
**O que mede:** "Das K recomendaÃ§Ãµes, quantas foram relevantes?"

**FÃ³rmula:**
```
Precision@10 = NÂº de hits / 10
```

**Exemplo:**
- Sistema recomenda 10 filmes
- UsuÃ¡rio assistiu 2 deles no conjunto de teste
- **Precision@10 = 0.20 (20%)**

**InterpretaÃ§Ã£o:**
- âœ… `> 0.10`: Bom
- âš ï¸ `0.05-0.10`: Moderado
- âŒ `< 0.05`: Ruim

---

### ğŸ¯ Recall@K
**O que mede:** "Dos filmes que o usuÃ¡rio gostou, quantos foram recomendados?"

**FÃ³rmula:**
```
Recall@10 = NÂº de hits / Total de filmes relevantes
```

**Exemplo:**
- UsuÃ¡rio tem 5 filmes no teste
- Sistema acertou 2 deles no top-10
- **Recall@10 = 0.40 (40%)**

**InterpretaÃ§Ã£o:**
- âœ… `> 0.30`: Bom
- âš ï¸ `0.15-0.30`: Moderado
- âŒ `< 0.15`: Ruim

---

### ğŸ¯ MAP@K (Mean Average Precision)
**O que mede:** "QuÃ£o bem os itens relevantes estÃ£o posicionados no ranking?"

**Por que Ã© importante:** Ã‰ a mÃ©trica mais importante! NÃ£o basta recomendar certo, precisa recomendar certo nas **primeiras posiÃ§Ãµes**.

**Exemplo:**
```
CenÃ¡rio A:
1. Matrix âœ“
2. Inception âœ“
MAP = Alto (relevantes no topo)

CenÃ¡rio B:
1. Filme X âœ—
2. Filme Y âœ—
...
9. Matrix âœ“
10. Inception âœ“
MAP = Baixo (relevantes no fim)
```

**InterpretaÃ§Ã£o:**
- âœ… `> 0.15`: Excelente
- âš ï¸ `0.10-0.15`: Bom
- âŒ `< 0.10`: Precisa melhorar

---

### ğŸ¯ NDCG@K (Normalized Discounted Cumulative Gain)
**O que mede:** "QuÃ£o prÃ³ximo o ranking estÃ¡ do ideal?"

**Conceito:** Itens em posiÃ§Ãµes mais baixas tÃªm desconto logarÃ­tmico.

**InterpretaÃ§Ã£o:**
- âœ… `> 0.25`: Excelente
- âš ï¸ `0.15-0.25`: Bom
- âŒ `< 0.15`: Precisa melhorar

---

### ğŸ¯ HitRate@K
**O que mede:** "Percentual de usuÃ¡rios que receberam pelo menos 1 recomendaÃ§Ã£o relevante"

**InterpretaÃ§Ã£o:**
- âœ… `> 0.70`: Ã“timo (70% dos usuÃ¡rios tÃªm hits)
- âš ï¸ `0.50-0.70`: Moderado
- âŒ `< 0.50`: Ruim

---

### ğŸ¯ Coverage
**O que mede:** "Diversidade do catÃ¡logo recomendado"

**FÃ³rmula:**
```
Coverage = NÂº de filmes recomendados / Total de filmes
```

**InterpretaÃ§Ã£o:**
- âœ… `> 0.20`: Boa diversidade
- âš ï¸ `0.10-0.20`: Moderada
- âŒ `< 0.10`: Muito focado em blockbusters

---

## ğŸ”¬ Experimentos Sugeridos

### 1. Impacto do NÃºmero de Caminhadas
```bash
python eval.py --num-walks 500
python eval.py --num-walks 1000
python eval.py --num-walks 2000
python eval.py --num-walks 5000
```

**HipÃ³tese:** Mais caminhadas = maior MAP, mas mais lento

---

### 2. Impacto do Comprimento da Caminhada
```bash
python eval.py --walk-length 5
python eval.py --walk-length 10
python eval.py --walk-length 15
python eval.py --walk-length 20
```

**HipÃ³tese:** Caminhadas mais longas exploram mais o grafo

---

### 3. Impacto da Nota MÃ­nima
```bash
python eval.py --min-user-rating 2.5
python eval.py --min-user-rating 3.0
python eval.py --min-user-rating 3.5
python eval.py --min-user-rating 4.0
```

**HipÃ³tese:** Notas mais altas = recomendaÃ§Ãµes mais precisas

---

### 4. ComparaÃ§Ã£o de Modos de Split
```bash
# Temporal (Ãºltimo filme)
python eval.py --split last --holdout 1

# AleatÃ³rio (20%)
python eval.py --split random --test-frac 0.2
```

**HipÃ³tese:** Split temporal Ã© mais realista

---

## ğŸ“ˆ Interpretando Resultados

### Exemplo de SaÃ­da

```
============================================================
  RESULTADOS DA AVALIACAO - RacoGraph (Random Walk)
============================================================

ğŸ“‹ ConfiguraÃ§Ã£o:
   Top-K               : 10
   Num Walks           : 1000
   Walk Length         : 10
   Min User Rating     : 3.0

ğŸ“Š MÃ©tricas de Qualidade:

   EstatÃ­sticas BÃ¡sicas:
   users_evaluated               : 610
   users_with_recs               : 598
   unique_movies_recommended     : 412

   MÃ©tricas de Ranking:
   Precision@10                  : 0.0845
   Recall@10                     : 0.4521
   MAP@10                        : 0.1289
   NDCG@10                       : 0.2145
   HitRate@10                    : 0.6721

   Outras MÃ©tricas:
   Coverage                      : 0.2187

============================================================

ğŸ’¡ InterpretaÃ§Ã£o:
   âœ… MAP: Excelente - modelo ranqueia bem itens relevantes
   âš ï¸  HitRate: Moderado - muitos usuÃ¡rios sem hits
   âœ… Coverage: Boa diversidade no catÃ¡logo
```

---

## ğŸ¯ Boas PrÃ¡ticas

### âœ… DO (FaÃ§a)

1. **Execute mÃºltiplas vezes** para ter certeza dos resultados
2. **Compare configuraÃ§Ãµes** de forma sistemÃ¡tica
3. **Documente parÃ¢metros** usados em cada experimento
4. **Analise trade-offs**: precisÃ£o vs. tempo de execuÃ§Ã£o
5. **Considere Coverage**: diversidade Ã© importante!

### âŒ DON'T (NÃ£o FaÃ§a)

1. **NÃ£o avalie apenas 1 mÃ©trica**: use MAP + HitRate + Coverage
2. **NÃ£o use apenas split aleatÃ³rio**: temporal Ã© mais realista
3. **NÃ£o ignore tempo**: 10000 walks pode ser muito lento
4. **NÃ£o otimize apenas para Precision**: pode sacrificar diversidade
5. **NÃ£o compare com dados de treino**: sempre use teste separado!

---

## ğŸ”§ Troubleshooting

### Problema: "MAP muito baixo (< 0.05)"
**SoluÃ§Ã£o:**
- Aumente `--num-walks`
- Aumente `--walk-length`
- Reduza `--min-user-rating`

### Problema: "HitRate muito baixo (< 0.40)"
**SoluÃ§Ã£o:**
- Verifique se grafo estÃ¡ bem conectado
- Aumente `--k` (avaliar top-20 em vez de top-10)
- Considere usar split aleatÃ³rio

### Problema: "Coverage muito baixa (< 0.10)"
**SoluÃ§Ã£o:**
- Sistema estÃ¡ enviesado para filmes populares
- Considere adicionar diversificaÃ§Ã£o no algoritmo
- Aumente `--walk-length` para explorar mais

### Problema: "AvaliaÃ§Ã£o muito lenta"
**SoluÃ§Ã£o:**
- Reduza `--num-walks` (teste com 500)
- Reduza `--walk-length` (teste com 5)
- Avalie subset de usuÃ¡rios para testes rÃ¡pidos

---

## ğŸ“š ReferÃªncias

- **Precision/Recall**: MÃ©tricas padrÃ£o de RecuperaÃ§Ã£o de InformaÃ§Ã£o
- **MAP**: PadrÃ£o em sistemas de ranqueamento
- **NDCG**: Microsoft Research, 2000
- **Random Walk**: Personalized PageRank (Page et al., 1998)

---

## ğŸ’¡ PrÃ³ximos Passos

ApÃ³s avaliar o sistema:

1. **Identifique pontos fracos** (qual mÃ©trica estÃ¡ baixa?)
2. **Teste hipÃ³teses** (o que pode melhorar?)
3. **Ajuste parÃ¢metros** (num_walks, walk_length, etc.)
4. **Re-avalie** e compare resultados
5. **Documente** configuraÃ§Ã£o final escolhida

---

**DÃºvidas?** Consulte o cÃ³digo em `eval.py` para detalhes de implementaÃ§Ã£o.
